{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPQknAYGNRE7bVfe/7R9fQ+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JeanMusenga/PhD-Thesis_2024_Musenga/blob/main/TextCNN_with_GloVe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://chatgpt.com/share/21220a6d-4cca-48bc-b461-bca6b0e140bc"
      ],
      "metadata": {
        "id": "Y9wwnMggWuD2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow"
      ],
      "metadata": {
        "id": "21Rjimu-T7Ua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "metadata": {
        "id": "QWlW0XM_T7Ry"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = './saved_file'\n",
        "file_path = ('ARPs_and_ProgrammingPosts.xlsx')\n",
        "data = pd.read_excel(file_path)"
      ],
      "metadata": {
        "id": "BV4tXXW9T7Pi"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip"
      ],
      "metadata": {
        "id": "ECQJX1X2iJWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess the data\n",
        "X = data['Question_body']\n",
        "y = data['Label']"
      ],
      "metadata": {
        "id": "1kVk2bp5_Wb_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)"
      ],
      "metadata": {
        "id": "Nb7fUNhF_WZQ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "7cBV9Yyr_WWp"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Load GloVe embeddings\n",
        "def load_glove_embeddings(glove_file):\n",
        "    embeddings_index = {}\n",
        "    with open(glove_file, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            coefs = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings_index[word] = coefs\n",
        "    return embeddings_index"
      ],
      "metadata": {
        "id": "bGGxAWCl_WUg"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you downloaded the 'glove.6B.100d.txt' file\n",
        "glove_file = 'glove.6B.100d.txt'\n",
        "embeddings_index = load_glove_embeddings(glove_file)\n",
        "print(f'Found {len(embeddings_index)} word vectors.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7LMr9ia_WRn",
        "outputId": "93c35aaa-46aa-436e-eaed-b4cee9c47b79"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 400000 word vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Tokenize Text and Create Embedding Matrix"
      ],
      "metadata": {
        "id": "xR8DRyl4DFpZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Tokenize the text data\n",
        "tokenizer = Tokenizer(num_words=20000)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# Create the embedding matrix\n",
        "embedding_dim = 100\n",
        "embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "# Pad the sequences\n",
        "max_length = 100\n",
        "X_train_padded = pad_sequences(tokenizer.texts_to_sequences(X_train), maxlen=max_length)\n",
        "X_test_padded = pad_sequences(tokenizer.texts_to_sequences(X_test), maxlen=max_length)\n"
      ],
      "metadata": {
        "id": "jQdAO3N6_WPI"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Create and Train TextCNN Model"
      ],
      "metadata": {
        "id": "4yLiD2vNDNPB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, Conv1D, MaxPooling1D, Flatten, Dense, Dropout, concatenate\n",
        "\n",
        "# Create the TextCNN model\n",
        "def create_text_cnn_model(vocab_size, embedding_dim, max_length, embedding_matrix):\n",
        "    inputs = Input(shape=(max_length,))\n",
        "    embedding = Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], input_length=max_length, trainable=False)(inputs)\n",
        "\n",
        "    conv1 = Conv1D(128, 3, activation='relu')(embedding)\n",
        "    pool1 = MaxPooling1D(pool_size=2)(conv1)\n",
        "\n",
        "    conv2 = Conv1D(128, 4, activation='relu')(embedding)\n",
        "    pool2 = MaxPooling1D(pool_size=2)(conv2)\n",
        "\n",
        "    conv3 = Conv1D(128, 5, activation='relu')(embedding)\n",
        "    pool3 = MaxPooling1D(pool_size=2)(conv3)\n",
        "\n",
        "    concatenated = concatenate([pool1, pool2, pool3], axis=1)\n",
        "    flatten = Flatten()(concatenated)\n",
        "    dense1 = Dense(128, activation='relu')(flatten)\n",
        "    dropout = Dropout(0.5)(dense1)\n",
        "    outputs = Dense(1, activation='sigmoid')(dropout)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "vocab_size = len(word_index) + 1\n"
      ],
      "metadata": {
        "id": "S3quSAcb_WMg"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the model\n",
        "text_cnn_model = create_text_cnn_model(vocab_size, embedding_dim, max_length, embedding_matrix)\n"
      ],
      "metadata": {
        "id": "_xUpWmik_WJ4"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the model summary\n",
        "text_cnn_model.summary()"
      ],
      "metadata": {
        "id": "zDc5THGb_WHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "history = text_cnn_model.fit(X_train_padded, y_train, epochs=10, batch_size=32, validation_data=(X_test_padded, y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HniCXShP_WEn",
        "outputId": "0ab99e4e-f14f-497d-8d79-ca47c7b92301"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "374/374 [==============================] - 62s 158ms/step - loss: 0.2956 - accuracy: 0.9025 - val_loss: 0.1793 - val_accuracy: 0.9387\n",
            "Epoch 2/10\n",
            "374/374 [==============================] - 47s 126ms/step - loss: 0.1595 - accuracy: 0.9460 - val_loss: 0.1690 - val_accuracy: 0.9394\n",
            "Epoch 3/10\n",
            "374/374 [==============================] - 54s 143ms/step - loss: 0.1130 - accuracy: 0.9601 - val_loss: 0.1926 - val_accuracy: 0.9381\n",
            "Epoch 4/10\n",
            "374/374 [==============================] - 44s 117ms/step - loss: 0.0621 - accuracy: 0.9773 - val_loss: 0.2194 - val_accuracy: 0.9277\n",
            "Epoch 5/10\n",
            "374/374 [==============================] - 55s 147ms/step - loss: 0.0304 - accuracy: 0.9887 - val_loss: 0.2826 - val_accuracy: 0.9361\n",
            "Epoch 6/10\n",
            "374/374 [==============================] - 49s 130ms/step - loss: 0.0276 - accuracy: 0.9911 - val_loss: 0.3116 - val_accuracy: 0.9337\n",
            "Epoch 7/10\n",
            "374/374 [==============================] - 55s 146ms/step - loss: 0.0189 - accuracy: 0.9946 - val_loss: 0.3967 - val_accuracy: 0.9357\n",
            "Epoch 8/10\n",
            "374/374 [==============================] - 46s 122ms/step - loss: 0.0202 - accuracy: 0.9933 - val_loss: 0.4060 - val_accuracy: 0.9371\n",
            "Epoch 9/10\n",
            "374/374 [==============================] - 52s 139ms/step - loss: 0.0172 - accuracy: 0.9947 - val_loss: 0.4120 - val_accuracy: 0.9361\n",
            "Epoch 10/10\n",
            "374/374 [==============================] - 48s 128ms/step - loss: 0.0099 - accuracy: 0.9972 - val_loss: 0.4228 - val_accuracy: 0.9327\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "loss, accuracy = text_cnn_model.evaluate(X_test_padded, y_test)\n",
        "print(f'Test Accuracy: {accuracy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fWT80pa_WB_",
        "outputId": "1f4bcec9-9c9c-4627-a2e2-f0735d8d0507"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "94/94 [==============================] - 3s 31ms/step - loss: 0.4228 - accuracy: 0.9327\n",
            "Test Accuracy: 0.9327083826065063\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on new data\n",
        "y_pred = text_cnn_model.predict(X_test_padded)\n",
        "y_pred_classes = (y_pred > 0.5).astype(\"int32\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQyE7ooh_V_h",
        "outputId": "ec3a0915-8b10-4a03-a2db-9d32fd7e7faa"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "94/94 [==============================] - 9s 87ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate predictions\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "report = classification_report(y_test, y_pred_classes)\n",
        "print(report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scpJWBWy_V9A",
        "outputId": "036e8d60-25df-4531-b940-1567c40a44c6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.92      0.93      1504\n",
            "           1       0.92      0.95      0.93      1483\n",
            "\n",
            "    accuracy                           0.93      2987\n",
            "   macro avg       0.93      0.93      0.93      2987\n",
            "weighted avg       0.93      0.93      0.93      2987\n",
            "\n"
          ]
        }
      ]
    }
  ]
}