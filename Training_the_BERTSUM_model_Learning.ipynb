{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPVSHSsMq2sISaaFOKRD1BU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JeanMusenga/PhD-Thesis_2024_Musenga/blob/main/Training_the_BERTSUM_model_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://chatgpt.com/share/7792533e-0081-430a-bec8-36c0c6f29067\n",
        "\n",
        "https://chatgpt.com/share/5b5e1beb-57f4-4b2d-b414-39e83b890ae9"
      ],
      "metadata": {
        "id": "lLVngpvz5KH9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the BERTSUM model\n",
        "The code for training a BERTSUM model is open-sourced by the researchers of BERTSUM and it is available at https://github.com/nlpyang/BertSum. In this section, let us explore this and learn how to train the BERTSUM model. We will train the BERTSUM model on the CNN/DailyMail news dataset.\n",
        "\n",
        "https://github.com/nlpyang/BertSum/blob/master/README.md\n",
        "\n",
        "Makse sure to run the notebook in GPU\n",
        "First, let us install the necessary libraries:"
      ],
      "metadata": {
        "id": "hbnz-c16z3NE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#If you are working with Google's colab, switch to the content directory with the following code:"
      ],
      "metadata": {
        "id": "VKecFHS-0Rz1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pytorch-pretrained-bert\n",
        "pip install tensorboardX\n",
        "pip install pyrouge"
      ],
      "metadata": {
        "id": "Gpp3uNHh4u70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9c1y8igful6N",
        "outputId": "6cb69880-068d-414c-d262-2faa0a91fd68"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-cxIsxM19M1",
        "outputId": "ea196bb6-efaa-425a-a150-a9e304496f4c"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "distributed.py  \u001b[0m\u001b[01;34mmodels\u001b[0m/  \u001b[01;34mothers\u001b[0m/  \u001b[01;34mprepro\u001b[0m/  preprocess.py  \u001b[01;34m__pycache__\u001b[0m/  train.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clone the BERTSUM repository:"
      ],
      "metadata": {
        "id": "0SmS7Jirz3KN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/nlpyang/BertSum.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8ruemctul3G",
        "outputId": "48914dc2-e3da-4ca8-89c9-10c2988979c7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'BertSum'...\n",
            "remote: Enumerating objects: 301, done.\u001b[K\n",
            "remote: Counting objects: 100% (293/293), done.\u001b[K\n",
            "remote: Compressing objects: 100% (124/124), done.\u001b[K\n",
            "remote: Total 301 (delta 165), reused 290 (delta 164), pack-reused 8\u001b[K\n",
            "Receiving objects: 100% (301/301), 15.05 MiB | 16.17 MiB/s, done.\n",
            "Resolving deltas: 100% (165/165), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now switch to the bert_data directory:"
      ],
      "metadata": {
        "id": "6jY7mij30ay1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/BertSum/bert_data/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2WpMZVOulyW",
        "outputId": "9545a9bc-b2b4-4d2e-db67-564e1d5f2f92"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/BertSum/bert_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The researchers have also made available the preprocessed CNN/DailyMail news dataset. So, first, let us download them:"
      ],
      "metadata": {
        "id": "E3dY47AW0hXt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from googleDriveFileDownloader import googleDriveFileDownloader\n",
        "gdrive = googleDriveFileDownloader()\n",
        "gdrive.downloadFile(\"https://drive.google.com/uc?id=1x0d61LP9UAN389YN00z0Pv-7jQgirVg6&export=download\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJzEkzvfulv1",
        "outputId": "bf31509c-50d5-4613-9f88-e5ebc8f8895c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download is starting\n",
            "FILENAME ::: bertsum_data.zip\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unzip the downloaded dataset:"
      ],
      "metadata": {
        "id": "O-m8B8kP0l42"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!unzip /content/BertSum/bert_data/bertsum_data.zip"
      ],
      "metadata": {
        "id": "m3ptY48yultV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Switch to src directory:"
      ],
      "metadata": {
        "id": "WpZ6LRb80owF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/BertSum/src"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4-rSi37ulq8",
        "outputId": "f54b886a-c8c4-45a8-97d5-7a8ab438e002"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/BertSum/src\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Now train the BERTSUM model\n",
        " In the following code, the argument -encoder classifier implies that we are training the BERTSUM model with a classifier:\n"
      ],
      "metadata": {
        "id": "x9DekH2Xvh_V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!python train.py -mode train -encoder classifier -dropout 0.1 -bert_data_path ../bert_data/cnndm -model_path ../models/bert_classifier -lr 2e-3 -visible_gpus 0 -gpu_ranks 0 -world_size 1 -report_every 50 -save_checkpoint_steps 1000 -batch_size 3000 -decay_method noam -train_steps 50 -accum_count 2 -log_file ../logs/bert_classifier -use_interval true -warmup_steps 10000"
      ],
      "metadata": {
        "id": "ZQlGj_xpuloU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py -mode train -encoder classifier -dropout 0.1 -bert_data_path ../bert_data/cnndm -model_path ../models/bert_classifier -lr 2e-3 -visible_gpus -1 -gpu_ranks -1 -world_size 1 -report_every 50 -save_checkpoint_steps 1000 -batch_size 3000 -decay_method noam -train_steps 50 -accum_count 2 -log_file ../logs/bert_classifier -use_interval true -warmup_steps 10000\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTmO2pAHxt1W",
        "outputId": "fccd2325-81db-4fc4-f7cd-02399bb56f2f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2024-08-02 15:48:10,528 INFO] Device ID -1\n",
            "[2024-08-02 15:48:10,528 INFO] Device cpu\n",
            "[2024-08-02 15:48:10,703 INFO] https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz not found in cache, downloading to /tmp/tmpt_jaq9b3\n",
            "100% 407873900/407873900 [00:12<00:00, 32790834.06B/s]\n",
            "[2024-08-02 15:48:23,391 INFO] copying /tmp/tmpt_jaq9b3 to cache at ../temp/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
            "[2024-08-02 15:48:24,934 INFO] creating metadata file for ../temp/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
            "[2024-08-02 15:48:24,935 INFO] removing temp file /tmp/tmpt_jaq9b3\n",
            "[2024-08-02 15:48:25,062 INFO] loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at ../temp/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
            "[2024-08-02 15:48:25,063 INFO] extracting archive file ../temp/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmp0b9aus5t\n",
            "[2024-08-02 15:48:30,411 INFO] Model config {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[2024-08-02 15:48:34,899 INFO] Summarizer(\n",
            "  (bert): Bert(\n",
            "    (model): BertModel(\n",
            "      (embeddings): BertEmbeddings(\n",
            "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "        (position_embeddings): Embedding(512, 768)\n",
            "        (token_type_embeddings): Embedding(2, 768)\n",
            "        (LayerNorm): BertLayerNorm()\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (encoder): BertEncoder(\n",
            "        (layer): ModuleList(\n",
            "          (0-11): 12 x BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): BertLayerNorm()\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): BertLayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pooler): BertPooler(\n",
            "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (activation): Tanh()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (encoder): Classifier(\n",
            "    (linear1): Linear(in_features=768, out_features=1, bias=True)\n",
            "    (sigmoid): Sigmoid()\n",
            "  )\n",
            ")\n",
            "gpu_rank 0\n",
            "[2024-08-02 15:48:34,910 INFO] * number of parameters: 109483009\n",
            "[2024-08-02 15:48:34,910 INFO] Start training...\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/BertSum/src/train.py\", line 340, in <module>\n",
            "    train(args, device_id)\n",
            "  File \"/content/BertSum/src/train.py\", line 272, in train\n",
            "    trainer.train(train_iter_fct, args.train_steps)\n",
            "  File \"/content/BertSum/src/models/trainer.py\", line 133, in train\n",
            "    train_iter = train_iter_fct()\n",
            "  File \"/content/BertSum/src/train.py\", line 253, in train_iter_fct\n",
            "    return data_loader.Dataloader(args, load_dataset(args, 'train', shuffle=True), args.batch_size, device,\n",
            "  File \"/content/BertSum/src/models/data_loader.py\", line 124, in __init__\n",
            "    self.cur_iter = self._next_dataset_iterator(datasets)\n",
            "  File \"/content/BertSum/src/models/data_loader.py\", line 145, in _next_dataset_iterator\n",
            "    self.cur_dataset = next(dataset_iter)\n",
            "  File \"/content/BertSum/src/models/data_loader.py\", line 99, in load_dataset\n",
            "    yield _lazy_dataset_loader(pt, corpus_type)\n",
            "  File \"/content/BertSum/src/models/data_loader.py\", line 83, in _lazy_dataset_loader\n",
            "    dataset = torch.load(pt_file)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 997, in load\n",
            "    with _open_file_like(f, 'rb') as opened_file:\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 444, in _open_file_like\n",
            "    return _open_file(name_or_buffer, mode)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 425, in __init__\n",
            "    super().__init__(open(name, mode))\n",
            "FileNotFoundError: [Errno 2] No such file or directory: '../bert_data/cnndm.train.pt'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to prepare data in BertSum format\n",
        "import pandas as pd\n",
        "import json\n",
        "def prepare_data_for_bertsum(data, output_file):\n",
        "    documents = []\n",
        "    for index, row in data.iterrows():\n",
        "        doc = {\"src\": row['Question_body'].split(), \"tgt\": row['Answer_body'].split()}\n",
        "        documents.append(doc)\n",
        "    with open(output_file, 'w') as f:\n",
        "        json.dump(documents, f)\n",
        "\n",
        "prepare_data_for_bertsum(data, 'data.json')"
      ],
      "metadata": {
        "id": "9cRrVevYYRy2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}