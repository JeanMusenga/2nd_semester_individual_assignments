{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM1H6SoZR5LDJNCAwQh2nzg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JeanMusenga/PhD-Thesis_2024_Musenga/blob/main/BERT_TextSummarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the BERT model for sequence classification, but adapted to perform a similar task of extractive summarization using transformers library\n",
        "https://chatgpt.com/share/01303bfd-981b-448e-b51c-4ac8bad51dc5"
      ],
      "metadata": {
        "id": "jsGmkbbqaYgg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "mSUQYmoIO3Nv"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Load the data"
      ],
      "metadata": {
        "id": "81aBEbbQa-Lf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_excel('DataSampePilot.xlsx')"
      ],
      "metadata": {
        "id": "Bt5muCYaO6--"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows with NaN in 'Question_body' or 'Answer_body'\n",
        "df = df.dropna(subset=['Question_body', 'Answer_body'])"
      ],
      "metadata": {
        "id": "ICmspuODQveO"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialize the summarization pipeline\n"
      ],
      "metadata": {
        "id": "p2Y8TBLabB_u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\", tokenizer=\"facebook/bart-large-cnn\")"
      ],
      "metadata": {
        "id": "BebsSqM9bDw1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function to split text into chunks smaller than max_length"
      ],
      "metadata": {
        "id": "4zrtptngbWVu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "sM1PUZWCOsqP"
      },
      "outputs": [],
      "source": [
        "# Function to split text into chunks smaller than max_length\n",
        "def split_into_chunks(text, max_length=512):\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    current_chunk = []\n",
        "    current_length = 0\n",
        "\n",
        "    for word in words:\n",
        "        if current_length + len(word) + 1 <= max_length:\n",
        "            current_chunk.append(word)\n",
        "            current_length += len(word) + 1\n",
        "        else:\n",
        "            chunks.append(\" \".join(current_chunk))\n",
        "            current_chunk = [word]\n",
        "            current_length = len(word) + 1\n",
        "\n",
        "    if current_chunk:\n",
        "        chunks.append(\" \".join(current_chunk))\n",
        "\n",
        "    return chunks"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Function to summarize text by splitting it into chunks"
      ],
      "metadata": {
        "id": "s4064xj6bbW3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to summarize text by splitting it into chunks\n",
        "def summarize_text(text, max_chunk_size=512):\n",
        "    chunks = split_into_chunks(text, max_chunk_size)\n",
        "\n",
        "    summaries = []\n",
        "    for chunk in chunks:\n",
        "        summary = summarizer(chunk, max_length=130, min_length=30, do_sample=False)[0]['summary_text']\n",
        "        summaries.append(summary)\n",
        "\n",
        "    combined_summary = ' '.join(summaries)\n",
        "    return combined_summary"
      ],
      "metadata": {
        "id": "vJNXvEEZTNQG"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Apply summarization to the 'Question_body' and 'Answer_body' columns"
      ],
      "metadata": {
        "id": "rAVw-F8_bdr2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply summarization to the 'Question_body' and 'Answer_body' columns\n",
        "df['Question_summary'] = df['Question_body'].apply(lambda x: summarize_text(x))\n",
        "df['Answer_summary'] = df['Answer_body'].apply(lambda x: summarize_text(x))"
      ],
      "metadata": {
        "id": "hhk2h6igRLUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Display the dataframe with summaries"
      ],
      "metadata": {
        "id": "mUE-o864bhVO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the dataframe with summaries\n",
        "print(df[['Question_body', 'Question_summary', 'Answer_body', 'Answer_summary']].head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nB0vSGdeRNpH",
        "outputId": "c09d5584-3375-48a1-da48-6e50a9d855be"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                       Question_body  \\\n",
            "0  Kinda new to AWS. I have this high-level quest...   \n",
            "1  I have some spring boot microservices and I wa...   \n",
            "2  I'm trying to properly design an application a...   \n",
            "3  I heard that for .NET8 Microsoft gifted us wit...   \n",
            "4  I am trying to learn AWS services, and now it ...   \n",
            "\n",
            "                                    Question_summary  \\\n",
            "0  I need a React app on the client, which is cal...   \n",
            "1  Spring boot microservices can be used to build...   \n",
            "2  I'm trying to properly design an application a...   \n",
            "3  Microsoft gave us a totally fixed authenticati...   \n",
            "4  I am trying to learn AWS services, and now it ...   \n",
            "\n",
            "                                         Answer_body  \\\n",
            "0  You send a request, you get a response. In ord...   \n",
            "1  <blockquote>\\ntl;dr: Spring MVC will not contr...   \n",
            "2  Determining the source of the information is b...   \n",
            "3  I have always asked myself this very same ques...   \n",
            "4  Short answer is: no, you don't have to but you...   \n",
            "\n",
            "                                      Answer_summary  \n",
            "0  Lambda is charged per miliseconds running. You...  \n",
            "1  Spring MVC will not contradict what you are tr...  \n",
            "2  Determining the source of the information is b...  \n",
            "3  Blazor Web Assembly uses Azure Entra External ...  \n",
            "4  Usually, EKS would be deployed in a private VP...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Save the dataframe with summaries to a new Excel file\n",
        "output_file_path = 'path_to_save_summarized_file/DataSampleSummarized.xlsx'  # Adjust this path as needed\n",
        "df.to_excel(output_file_path, index=False)"
      ],
      "metadata": {
        "id": "bDcccYKvROEv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}