{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPY3CIu4/DCsQMSxfDNKyOa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JeanMusenga/PhD-Thesis_2024_Musenga/blob/main/TextCNN_with_TF_IDF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://chatgpt.com/share/727dee06-b960-4baa-b017-d4ac62e8d326"
      ],
      "metadata": {
        "id": "MmoZuuOQ3kGV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, concatenate, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "oRErRT-k26n5"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the dataset"
      ],
      "metadata": {
        "id": "4sD-2OLV3mD5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "file_path = 'posts.xlsx'  # Replace with the actual file path\n",
        "data = pd.read_excel(file_path)"
      ],
      "metadata": {
        "id": "Q8jHmeJ234_w"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocess text data"
      ],
      "metadata": {
        "id": "hpZj4n6r3rdA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess text data\n",
        "data['Question_body'] = data['Question_body'].str.replace('\\n', ' ').str.replace('<.*?>', '', regex=True)\n",
        "\n",
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data['Question_body'], data['Label'], test_size=0.3, random_state=42)\n"
      ],
      "metadata": {
        "id": "I_bgYsSY3BZA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compute TF-IDF features"
      ],
      "metadata": {
        "id": "zQ_9gIdp3tBW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute TF-IDF features\n",
        "vectorizer = TfidfVectorizer(max_features=10000)\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# Reshape TF-IDF features to fit CNN input shape\n",
        "X_train_cnn = X_train_tfidf.toarray().reshape((X_train_tfidf.shape[0], X_train_tfidf.shape[1], 1))\n",
        "X_test_cnn = X_test_tfidf.toarray().reshape((X_test_tfidf.shape[0], X_test_tfidf.shape[1], 1))"
      ],
      "metadata": {
        "id": "IPNwBOUF31M3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define the TextCNN model"
      ],
      "metadata": {
        "id": "-ov764zv39RY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the TextCNN model\n",
        "def create_text_cnn_model(input_shape):\n",
        "    inputs = Input(shape=input_shape)\n",
        "\n",
        "    conv1 = Conv1D(128, 3, activation='relu')(inputs)\n",
        "    pool1 = MaxPooling1D(pool_size=2)(conv1)\n",
        "\n",
        "    conv2 = Conv1D(128, 4, activation='relu')(inputs)\n",
        "    pool2 = MaxPooling1D(pool_size=2)(conv2)\n",
        "\n",
        "    conv3 = Conv1D(128, 5, activation='relu')(inputs)\n",
        "    pool3 = MaxPooling1D(pool_size=2)(conv3)\n",
        "\n",
        "    concatenated = concatenate([pool1, pool2, pool3], axis=1)\n",
        "    flatten = Flatten()(concatenated)\n",
        "    dense1 = Dense(128, activation='relu')(flatten)\n",
        "    dropout = Dropout(0.5)(dense1)\n",
        "    outputs = Dense(1, activation='sigmoid')(dropout)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "JMBqr_m83J54"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create the model"
      ],
      "metadata": {
        "id": "Wj38D0yc4B4x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the model\n",
        "input_shape = (X_train_cnn.shape[1], 1)\n",
        "model = create_text_cnn_model(input_shape)\n"
      ],
      "metadata": {
        "id": "0JJ6yTJp3MVQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the model"
      ],
      "metadata": {
        "id": "I1h86_BF4EC4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "history = model.fit(X_train_cnn, y_train, epochs=10, batch_size=32, validation_split=0.2, callbacks=[early_stopping])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPtGTSUx3OO4",
        "outputId": "13e97fb6-d0ad-425c-8d87-2caa9ffd9713"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 159ms/step - accuracy: 0.9385 - loss: 0.1563 - val_accuracy: 0.9024 - val_loss: 0.2622\n",
            "Epoch 2/10\n",
            "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 149ms/step - accuracy: 0.9639 - loss: 0.0967 - val_accuracy: 0.8967 - val_loss: 0.3352\n",
            "Epoch 3/10\n",
            "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 149ms/step - accuracy: 0.9875 - loss: 0.0351 - val_accuracy: 0.8996 - val_loss: 0.3546\n",
            "Epoch 4/10\n",
            "\u001b[1m262/262\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 148ms/step - accuracy: 0.9946 - loss: 0.0179 - val_accuracy: 0.8962 - val_loss: 0.4199\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test set\n",
        "test_loss, test_accuracy = model.evaluate(X_test_cnn, y_test)\n",
        "print(f'Test Accuracy: {test_accuracy:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDUdqCtB3XeI",
        "outputId": "e9e7441d-90ba-449e-da5c-93564e71fec0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.9060 - loss: 0.2588\n",
            "Test Accuracy: 0.9045\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVX9DmKq2oG5",
        "outputId": "50ea0214-c166-4676-cc49-81e6379ba599"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m140/140\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step\n",
            "Class 0 - Precision: 0.9169314571039492, Recall: 0.891832229580574, Accuracy: 0.9044642857142857, F1-score: 0.9042076991942704, Support: 2265\n",
            "Class 1 - Precision: 0.8924022837066315, Recall: 0.9173814898419864, Accuracy: 0.9044642857142857, F1-score: 0.9047195013357079, Support: 2215\n"
          ]
        }
      ],
      "source": [
        "# Predict on the test set\n",
        "y_pred_probs = model.predict(X_test_cnn)\n",
        "y_pred = (y_pred_probs > 0.5).astype(int)\n",
        "\n",
        "# Compute and display metrics\n",
        "precision_class, recall_class, f1_class, support_class = precision_recall_fscore_support(y_test, y_pred, average=None, labels=[0, 1])\n",
        "conf_matrix = confusion_matrix(y_test, y_pred, labels=[0, 1])\n",
        "\n",
        "# Calculate overall accuracy\n",
        "accuracy = (conf_matrix[0, 0] + conf_matrix[1, 1]) / conf_matrix.sum()\n",
        "\n",
        "print(f'Class 0 - Precision: {precision_class[0]}, Recall: {recall_class[0]}, Accuracy: {accuracy}, F1-score: {f1_class[0]}, Support: {support_class[0]}')\n",
        "print(f'Class 1 - Precision: {precision_class[1]}, Recall: {recall_class[1]}, Accuracy: {accuracy}, F1-score: {f1_class[1]}, Support: {support_class[1]}')\n"
      ]
    }
  ]
}